Nueron - processes a data point
Layer - group of neurons that process similar points
Output Layer - output layer from another layer (desired output)
Input Layaer - features, data input, etc..
Hidden Layer - layer in the middle (some value in between x and y)

Super script - means the layer we're on (oh yeah more indices)
Sum of layers - sum of hidden + output (1)

a_j [l] = g(w_j[l] . a[l-1] + b_j[l]) = g(z)

forward propogation - moving left to right
unit - index of the layer (j above)

later_1 = Dense(unit = 3, activation='sigmong')
layer_2 = Dense(unti = 1, activation='sigmong)
model = Sequential([layer_1, layer_2])

model.compile() - specify the loss function
model.fit(x,y) - eposh (number of steps)
model.predict(x_new)

ANI = artifial narrow intelligence, narrow task
AGI = artifical general intelligence, do anything a human can do

Steps:
1 - Create the model
2 - Loss and cost functions
  + BinaryCrossentropy() - log loss (yes/no)
  + MeadSquaredError() - linear loss
3 -  Gradient Descent (minimize loss duhhh) (model.fit) - eposh (number of iterations)

Activation functions
--------------------
Why use sigmoid - because we wewre doing a lot of log regression
Others
- Linear - Linear activiation function (no activation)
     a = g(z) = w.x + b = z
- ReLU - rectified linear unit 
    (if z < 0 -> g(z) is 0)
    (if z >= 0 -> g(z) is z)

Output Layer Type
If binary classification - use sigmoid as output layer (duhh)
If it can be negative or positive - use linear as output (duhh)
 - e.g. stock change from yesterday
If it can only be positive - use ReLU (duhhhh)
 - e.g. price of a house

Hidden Layer Type
Use ReLU - most common, faster, less flat so the gradient descent will go slower duhh

Why need activation functions?
 - if we don't then no different than linear regression

MultiClassification
-------------------
Multiple classifications (not just binary - yes/no, can take on multiple values)
 - decision boundary that devides outputs into groups

Softmax - generalization of logistic regression
 - ex. 4 outputs (1,2,3,4) => p(y=1) = a1 = e^z1 / e^z1 + e^z2 + etc.
 - aj = e^Zj / sum k..N e^Zk

Loss = -log(aN) y = N
 - crossentropy loss

Use as output layer (duhh)
 - Dense(units=10, activation='softmax') - output
 - model.compile(loss = SparseCategoricalCrossentropy) - loss

Even better way..
 - why? - numerical roundoff errors (tensorflow rearranges equations)
- use from_logits = True in loss function

MultiLabel Classification - output can have multiple labels (e.g. 1,2 and 3)

Advanced Optimization
---------------------
Better cost functions than gradient descent?!

Adam Algorithm - adjusts learning rate as it goes (craziness)
 - Different learning rate for each param of the model
 - model.compile(optimizer=tf.eras.optimizer.Adam(learning_rate=1e-3))
 - defacto standard

Other Layer Types
-----------------
Dense - what we've been using
 - each neuron output is a function of all the activation outputs of the previous layer

Convolutional Layer -each neuron is a function of subset activation outputs of the previous layer
 - faster
 - needs less data
 - less prone to overfitting

Debugging...
------------
diagnostic - a test you can run to see what's going wrong

method 1 - 
 prob - notice overfitting
 soln - split data set into subsets 
  - 1 training set
  - 2 test set
  - compare the loss of training set vs test set 
    - if training set loss is low and test set is high -> not good
      - that means it does not handle new data well at all
  - NOTE: for classiciation (compare misclassifications)

method 2
 - split it 3 subsets 
  - 1: training set (e.g. 60%)
  - 2: cross validation set (e.g. 20%)
  - 3: test set (e.g. 20%)


Variance/Bias
-------------
TODO: add other notes..
- neural networks are low bias machines


The How To...
1 - Choose Architecture
2 - training
3 - Diagnose
4 - Repeat until convergence ;)


Data Augmentation - modifying an existing training example to create a new one.
Data Synthesis - creating new data from scratch (not from existing)
Transfer Learning - chnage output layer of other model with yours (:0)

Cycle:
1 - Define Project
2 - Define and collect data
3 - Training, error analysis & iterative improvement
4 - deploy, monitor, and maintain system


Decision Trees
--------------
root node - top node (duhh)
Algorithm - determines best tree 
