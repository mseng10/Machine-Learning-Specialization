unsupervised learning - ask the alg. to find commonalities in data
clustering - find if the data can be grouped

k meanns
 - intiial guess at cluster centroids
 - see which ones are closest 
 - pick new centroids until convergence 


k-means alg.
 1) Randomly intiialize K cluster centroids (k is number axis)
 2) for i to  m: find which ones are cloesest
 3) Move cluster centroids
 
cost function of k means - minimize average square distance for datapoints to their cluster

intiializing k-means
 - choose K < m 
 1) randomly pick K training examples
 2) Set u_1..k equal to these K examples (bing bong)
 3) Run different times to pick best intiializer
  - minimizing cluster centroids 

anomoly detection

Gaussian (Normal) Detection - duhhhhhhh
 - mean, variance ,etc.

density estimation - find features of x that have low/high probaility
 - p(x) < epsilon
 - p(x) = p(x_1;u_1,o_1) *... p(x_n;u_n,o_n) (assumes independadnt)
 
1) Choose n features
2) fit parameters
3) p(x) = sum (p(x_j;u_j,o_j))

anomalous - rare or ananomly

could train on dataset with good/bad to get episolon

anomoly vs supervised
 - anomoly 
   + small number of positive examples
   + large number of negative examples
   + many different positive examples (e.g. many things can go wrong in air craft enginge)
   + used to help find new issues (never before seen)

anomoly feature choices
 - ensure feature are gaussian (or change them to be more gaussian)
   + plt.hist(x)
 - error ananlysis - see where alg. is not doing so good
   + p(x) >= e is large for Normal
   + common error -> p(x) is large for both
 
Recommender Systems
-------------------
can use linear regression to suggest values for a user (review)


what happens if we don't have quantitfyig values ? 
--> collaborative filtering alg.
 - use x features to determine who rates what as what
   + little scary we give this to the user... 

You can join the 2 cost functions of learning the features and params to a single cost function
 - make w,b,x parameters!
ex) movie rating system where people have rated

with binary data --> just use logistic regresion!
 - duhhhhhhh

mean nornalization - use it to run quicker/cleaner for collaborative filtering
 - 0 row mean

Content Based Filtering - recommend items based on features of user and item to find a good match
 - not ratings of users (which is a feature... odd)
 - e.g. age of user, year movie was made, etc
 - use deep learning!

turn x features in a vector that can "summarize the x features"
take vectors of both sets of features and dot product them together
features matters!!!
computationally expensive if a large catelog of data x features



Resinforcement Learning 
 - reward feedback loop
 - state -> action

 return - what you get from taking a set of actions (worth)
 discount rate - like interest, 
 policy - maps state to action
 QAS - ppssihle actions at a state
  - 1: start in state s
  - 2: take action a 
  - 3: 
Bellman Equation - Q(s,a) = R(s) + (y) * max Q(s',a')
 - s', a' - state and action in next state
 - R - return of current state
 - y = discount rate

