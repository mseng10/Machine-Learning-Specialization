unsupervised learning - ask the alg. to find commonalities in data
clustering - find if the data can be grouped

k meanns
 - intiial guess at cluster centroids
 - see which ones are closest 
 - pick new centroids until convergence 


k-means alg.
 1) Randomly intiialize K cluster centroids (k is number axis)
 2) for i to  m: find which ones are cloesest
 3) Move cluster centroids
 
cost function of k means - minimize average square distance for datapoints to their cluster

intiializing k-means
 - choose K < m 
 1) randomly pick K training examples
 2) Set u_1..k equal to these K examples (bing bong)
 3) Run different times to pick best intiializer
  - minimizing cluster centroids 

anomoly detection

Gaussian (Normal) Detection - duhhhhhhh
 - mean, variance ,etc.

density estimation - find features of x that have low/high probaility
 - p(x) < epsilon
 - p(x) = p(x_1;u_1,o_1) *... p(x_n;u_n,o_n) (assumes independadnt)
 
1) Choose n features
2) fit parameters
3) p(x) = sum (p(x_j;u_j,o_j))

anomalous - rare or ananomly

could train on dataset with good/bad to get episolon

anomoly vs supervised
 - anomoly 
   + small number of positive examples
   + large number of negative examples
   + many different positive examples (e.g. many things can go wrong in air craft enginge)
   + used to help find new issues (never before seen)

anomoly feature choices
 - ensure feature are gaussian (or change them to be more gaussian)
   + plt.hist(x)
 - error ananlysis - see where alg. is not doing so good
   + p(x) >= e is large for Normal
   + common error -> p(x) is large for both
 